{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4C0z5vjtjYb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b272b281-0006-4435-aa8f-af05858831a6",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for chroma (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m527.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.7/407.7 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.9/296.9 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.7/249.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m607.0/607.0 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.8/255.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-core 0.3.12 requires langsmith<0.2.0,>=0.1.125, but you have langsmith 0.0.92 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m846.5/846.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "types-requests 2.32.0.20241016 requires urllib3>=2, but you have urllib3 1.26.20 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.5/294.5 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hReading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following additional packages will be installed:\n",
            "  fonts-droid-fallback fonts-noto-mono fonts-urw-base35 libgs9 libgs9-common\n",
            "  libidn12 libijs-0.35 libjbig2dec0 poppler-data\n",
            "Suggested packages:\n",
            "  fonts-noto fonts-freefont-otf | fonts-freefont-ttf fonts-texgyre\n",
            "  ghostscript-x poppler-utils fonts-japanese-mincho | fonts-ipafont-mincho\n",
            "  fonts-japanese-gothic | fonts-ipafont-gothic fonts-arphic-ukai\n",
            "  fonts-arphic-uming fonts-nanum\n",
            "The following NEW packages will be installed:\n",
            "  fonts-droid-fallback fonts-noto-mono fonts-urw-base35 ghostscript libgs9\n",
            "  libgs9-common libidn12 libijs-0.35 libjbig2dec0 poppler-data\n",
            "0 upgraded, 10 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 16.7 MB of archives.\n",
            "After this operation, 63.0 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-droid-fallback all 1:6.0.1r16-1.1build1 [1,805 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 poppler-data all 0.4.11-1 [2,171 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-noto-mono all 20201225-1build1 [397 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-urw-base35 all 20200910-1 [6,367 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9-common all 9.55.0~dfsg1-0ubuntu5.9 [752 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libidn12 amd64 1.38-4ubuntu1 [60.0 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libijs-0.35 amd64 0.35-15build2 [16.5 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjbig2dec0 amd64 0.19-3build2 [64.7 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9 amd64 9.55.0~dfsg1-0ubuntu5.9 [5,033 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ghostscript amd64 9.55.0~dfsg1-0ubuntu5.9 [49.5 kB]\n",
            "Fetched 16.7 MB in 1s (30.9 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 10.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-droid-fallback.\n",
            "(Reading database ... 123622 files and directories currently installed.)\n",
            "Preparing to unpack .../0-fonts-droid-fallback_1%3a6.0.1r16-1.1build1_all.deb ...\n",
            "Unpacking fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\n",
            "Selecting previously unselected package poppler-data.\n",
            "Preparing to unpack .../1-poppler-data_0.4.11-1_all.deb ...\n",
            "Unpacking poppler-data (0.4.11-1) ...\n",
            "Selecting previously unselected package fonts-noto-mono.\n",
            "Preparing to unpack .../2-fonts-noto-mono_20201225-1build1_all.deb ...\n",
            "Unpacking fonts-noto-mono (20201225-1build1) ...\n",
            "Selecting previously unselected package fonts-urw-base35.\n",
            "Preparing to unpack .../3-fonts-urw-base35_20200910-1_all.deb ...\n",
            "Unpacking fonts-urw-base35 (20200910-1) ...\n",
            "Selecting previously unselected package libgs9-common.\n",
            "Preparing to unpack .../4-libgs9-common_9.55.0~dfsg1-0ubuntu5.9_all.deb ...\n",
            "Unpacking libgs9-common (9.55.0~dfsg1-0ubuntu5.9) ...\n",
            "Selecting previously unselected package libidn12:amd64.\n",
            "Preparing to unpack .../5-libidn12_1.38-4ubuntu1_amd64.deb ...\n",
            "Unpacking libidn12:amd64 (1.38-4ubuntu1) ...\n",
            "Selecting previously unselected package libijs-0.35:amd64.\n",
            "Preparing to unpack .../6-libijs-0.35_0.35-15build2_amd64.deb ...\n",
            "Unpacking libijs-0.35:amd64 (0.35-15build2) ...\n",
            "Selecting previously unselected package libjbig2dec0:amd64.\n",
            "Preparing to unpack .../7-libjbig2dec0_0.19-3build2_amd64.deb ...\n",
            "Unpacking libjbig2dec0:amd64 (0.19-3build2) ...\n",
            "Selecting previously unselected package libgs9:amd64.\n",
            "Preparing to unpack .../8-libgs9_9.55.0~dfsg1-0ubuntu5.9_amd64.deb ...\n",
            "Unpacking libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.9) ...\n",
            "Selecting previously unselected package ghostscript.\n",
            "Preparing to unpack .../9-ghostscript_9.55.0~dfsg1-0ubuntu5.9_amd64.deb ...\n",
            "Unpacking ghostscript (9.55.0~dfsg1-0ubuntu5.9) ...\n",
            "Setting up fonts-noto-mono (20201225-1build1) ...\n",
            "Setting up libijs-0.35:amd64 (0.35-15build2) ...\n",
            "Setting up fonts-urw-base35 (20200910-1) ...\n",
            "Setting up poppler-data (0.4.11-1) ...\n",
            "Setting up libjbig2dec0:amd64 (0.19-3build2) ...\n",
            "Setting up libidn12:amd64 (1.38-4ubuntu1) ...\n",
            "Setting up fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\n",
            "Setting up libgs9-common (9.55.0~dfsg1-0ubuntu5.9) ...\n",
            "Setting up libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.9) ...\n",
            "Setting up ghostscript (9.55.0~dfsg1-0ubuntu5.9) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "!pip install chroma -q\n",
        "!pip install --upgrade langchain -q\n",
        "!pip install openai==0.28 -q\n",
        "!pip install tiktoken -q\n",
        "!pip install cohere -q\n",
        "!pip install chromadb -q\n",
        "!pip install -U sentence-transformers -q\n",
        "!pip install langchain==0.0.340 -q\n",
        "!pip install llama-index==0.8.36 -q\n",
        "!pip install PyPDF2 -q\n",
        "!pip install faiss-cpu -q\n",
        "!pip install --upgrade fastapi -q\n",
        "!pip install typing_extensions -q\n",
        "!pip install langchain -q\n",
        "!pip install llama-index -q\n",
        "!pip install camelot-py -q\n",
        "!pip install ghostscript -q\n",
        "!pip install pypdf -q\n",
        "!pip install tabula-py -q\n",
        "!sudo apt-get install ghostscript -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show llama_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNXJEXK3WPdA",
        "outputId": "643cedd3-c199-4de3-b8f7-6afcf5a75f89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: llama-index\n",
            "Version: 0.8.36\n",
            "Summary: Interface between LLMs and your data\n",
            "Home-page: https://github.com/jerryjliu/llama_index\n",
            "Author: Jerry Liu\n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: beautifulsoup4, dataclasses-json, fsspec, langchain, nest-asyncio, nltk, numpy, openai, pandas, sqlalchemy, tenacity, tiktoken, typing-extensions, typing-inspect, urllib3\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import GPTVectorStoreIndex"
      ],
      "metadata": {
        "id": "JAv0nKlbVsoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWKxGTKRVx_H"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "from langchain.document_loaders import TextLoader\n",
        "import os\n",
        "import logging\n",
        "import uuid\n",
        "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "import uuid\n",
        "import os\n",
        "import logging\n",
        "from langchain.prompts import PromptTemplate\n",
        "import PyPDF2\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "#from llama_index.schema import Document\n",
        "from langchain.schema.document import Document\n",
        "import openai\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from tabula.io import read_pdf\n",
        "import os\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "from io import BytesIO\n",
        "import camelot\n",
        "import tabula\n",
        "import pandas as pd\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "import openpyxl\n",
        "from langchain.chains import  RetrievalQA\n",
        "\n",
        "\n",
        "\n",
        "user_index_file = 'vector/chroma'\n",
        "user_path = \"/vector/\"\n",
        "os.environ[\"OPENAI_API_KEY\"] =  \"replace it with your key\"\n",
        "embeddings  =   OpenAIEmbeddings()\n",
        "llm = OpenAI(temperature=0.6,model=\"gpt-3.5-turbo-instruct-0914\")\n",
        "memory = ConversationBufferMemory()\n",
        "conversation_chain = ConversationChain(llm=llm,memory = memory)\n",
        "import re\n",
        "\n",
        "def df_col_adjust(df):\n",
        "  for i in range(len(df.columns)):\n",
        "    if not df.columns[i].strip():  # Check if the column name is empty\n",
        "        df.rename(columns={df.columns[i]: df.columns[i-1]}, inplace=True)\n",
        "  while df.iloc[0].str.strip().eq('').any() or df.iloc[0].isnull().any():\n",
        "    df.columns = [col if pd.isna(df.iloc[0][i]) else f'{col}-{df.iloc[0][i]}' for i, col in enumerate(df.columns)]\n",
        "    df = df.iloc[1:]\n",
        "  return df\n",
        "\n",
        "def df_to_text(df):\n",
        "        df = df_col_adjust(df)\n",
        "        # Initialize an empty list to store the formatted text lines\n",
        "        text_lines = []\n",
        "\n",
        "        # Iterate through the DataFrame rows\n",
        "        for index, row in df.iterrows():\n",
        "            # Format each record with its respective heading\n",
        "            record_text = \"\"\n",
        "            for column, value in row.items():\n",
        "              column = re.sub(r'\\n+', ' - ', column)\n",
        "              column = re.sub(r'\\s+', ' ', column)\n",
        "              value = value.replace('\\n', ' ')\n",
        "              value = re.sub(r'\\s+', ' ', value)\n",
        "              value = re.sub(r'\\n+', ' - ', value)\n",
        "              value = value.strip()\n",
        "              record_text += f\"{column}: {value}\"\n",
        "              record_text += \"\\n\"\n",
        "\n",
        "            text_lines.append(record_text)\n",
        "\n",
        "        # Combine the formatted lines into a single text string\n",
        "        formatted_text = \"\\n\".join(text_lines)\n",
        "\n",
        "        return formatted_text\n",
        "\n",
        "def check(df):\n",
        "  from tabulate import tabulate\n",
        "  return tabulate(df, headers = 'keys', tablefmt = 'psql')\n",
        "\n",
        "# Now, text_lines contains the formatted rows with both column names and values split by '\\n'\n",
        "\n",
        "\n",
        "\n",
        "def extract_table_from_pdf(pdf_path,page_number):\n",
        "    try:\n",
        "      tables=camelot.read_pdf(pdf_path,pages=str(page_number))\n",
        "      print(f\"Extracting tables from page {page_number}\")\n",
        "\n",
        "      print(\"-------tables---\")\n",
        "      print(tables)\n",
        "      print(\"----------\")\n",
        "      # Check if any tables were extracted and display them\n",
        "      if len(tables) != 0:\n",
        "        text = \"\"\n",
        "        for table in tables:\n",
        "          try:\n",
        "            df = table.df\n",
        "            print(\"--------------1-----------------\")\n",
        "\n",
        "            is_header = not any(str(cell).isdigit() for cell in df.columns)\n",
        "            if not is_header:\n",
        "              df = df.rename(columns=df.iloc[0]).drop(df.index[0])\n",
        "              print(\"--------------2-----------------\")\n",
        "            text += df_to_text(df)\n",
        "            print(\"Extracted Table:\")\n",
        "            print(df)\n",
        "            embeddings = embedding_function.encode(df_to_text(df))\n",
        "            print(\"Embeddings:\")\n",
        "            print(embeddings)\n",
        "\n",
        "          except Exception as e:\n",
        "            print(f\"An error occurred: {str(e)}\")\n",
        "            continue\n",
        "        return text\n",
        "      return ''\n",
        "    except Exception as e:\n",
        "        # print(df)\n",
        "        # Handle exceptions and return an empty string\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        return ''\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        file_bytes=file.read()\n",
        "        text = ''\n",
        "        for page_number, page in enumerate(reader.pages, start=1):\n",
        "          text += \"\\n PAGE_START \\n\"\n",
        "          text += page.extract_text()\n",
        "\n",
        "          table_text=extract_table_from_pdf(pdf_path,page_number)\n",
        "          print(table_text)\n",
        "          text += \"\\n---TABLE DATA---\\n\"\n",
        "          text += table_text\n",
        "          print(\"-------------------------------\")\n",
        "          text += \"\\n---table_data_end-----\\n\"\n",
        "\n",
        "          text += \"\\n PAGE_END \\n\"\n",
        "    return text\n",
        "\n",
        "# def extract_text_from_pdf(pdf_path):\n",
        "#     with open(pdf_path, 'rb') as file:\n",
        "#         reader = PyPDF2.PdfReader(file)\n",
        "#         file_bytes=file.read()\n",
        "#         text = ''\n",
        "#         for page_number, page in enumerate(reader.pages, start=1):\n",
        "#           # text += \"\\n PAGE_START \\n\"\n",
        "#           text += page.extract_text()\n",
        "\n",
        "#           table_text=extract_table_from_pdf(pdf_path,page_number)\n",
        "#           # print(table_text)\n",
        "#           # text += \"\\n---TABLE DATA---\\n\"\n",
        "#           if table_text != '':\n",
        "#             text+=\"\\n-----find info for above text--------\\n\"\n",
        "#             text += table_text\n",
        "#           print(\"-------------------------------\")\n",
        "#           text += \"\\n----\\n\"\n",
        "\n",
        "#           text += \"\\n \\n\\n \\n\"\n",
        "#     return text\n",
        "\n",
        "\n",
        "\n",
        "# def split_text_into_chunks(text, chunk_size,separator='\\n'):\n",
        "#     text_splitter = CharacterTextSplitter(\n",
        "#         separator=separator,\n",
        "#         chunk_size=chunk_size,\n",
        "#         chunk_overlap=0\n",
        "#     )\n",
        "#     chunks = text_splitter.split_text(text)\n",
        "#     # for i, chunk in enumerate(chunks):\n",
        "#     #     print(f\"Chunk {i + 1}:\")\n",
        "#     #     print(chunk)\n",
        "#     # print(chunks)\n",
        "#     # num_chunks = len(chunks)\n",
        "#     return chunks\n",
        "\n",
        "# def split_text_into_chunks(text, chunk_size,separator='\\n'):\n",
        "#     text_splitter = RecursiveCharacterTextSplitter(\n",
        "#         separator=separator,\n",
        "#         chunk_size=chunk_size,\n",
        "#         chunk_overlap=0\n",
        "#     )\n",
        "#     chunks = text_splitter.split_text(text)\n",
        "#     print(chunks)\n",
        "\n",
        "def split_text_into_chunks(text, chunk_size=3000, chunk_overlap=300, separator='\\n'):\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        separator=separator,\n",
        "        is_separator_regex=False,  # Change this if needed based on the library's requirements\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    return chunks\n",
        "\n",
        "def process_text_chunks(text_chunks):\n",
        "    for chunk_index, chunk_text in enumerate(text_chunks, start=1):\n",
        "        # Process each chunk as needed\n",
        "        print(chunk_text)\n",
        "\n",
        "def get_text_chunks(text):\n",
        "    separator = '\\n\\n'\n",
        "    separator = \"\"\n",
        "    # Split text into chunks with specified parameters\n",
        "    chunks = split_text_into_chunks(text, chunk_size=3000, chunk_overlap=300, separator=separator)\n",
        "    return chunks\n",
        "\n",
        "def get_documents_from_text(text_chunks, file_name, file_id):\n",
        "    docs = []\n",
        "    for chunk_index, chunk_text in enumerate(text_chunks, start=1):\n",
        "        doc = Document(page_content=chunk_text, metadata={\"source\": file_name, \"file_id\": f\"{file_id}_chunk_{chunk_index}\"})\n",
        "        docs.append(doc)\n",
        "    return docs\n",
        "# def get_documents_from_text( input_text, file_name, file_id):\n",
        "#     documents = SimpleDirectoryReader(file_id).load_data()\n",
        "#     docs = []\n",
        "#     for doc in documents:\n",
        "#         docs.append(Document(page_content=doc.text, metadata={\"source\": file_name,  \"file_id\": file_id}))\n",
        "#     # for doc in documents:\n",
        "#     #     print(doc)\n",
        "#     #     doc.metadata={\"source\": file_name, \"data_source_id\": data_source_id, \"file_id\": file_id}\n",
        "#     return docs\n",
        "\n",
        "\n",
        "def create_vector_index(input_text, file_name, file_id):\n",
        "    chunks = split_text_into_chunks(input_text, chunk_size=3000, chunk_overlap=300, separator='\\n')\n",
        "    documents = get_documents_from_text(chunks, file_name, file_id)\n",
        "\n",
        "    # Create the open-source embedding function\n",
        "    embedding_model_name = \"gtr-t5-large\"\n",
        "    embedding_function = SentenceTransformerEmbeddings(model_name=embedding_model_name)\n",
        "\n",
        "    index_file = user_index_file\n",
        "\n",
        "    vectordb = Chroma.from_documents(\n",
        "        documents=documents,\n",
        "        embedding=embedding_function,\n",
        "        persist_directory=index_file\n",
        "    )\n",
        "    return vectordb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqtWVkxrt052"
      },
      "outputs": [],
      "source": [
        "def create_retriver():\n",
        "  vector = create_vector_index(input_text, file_name, file_id)\n",
        "  retriever = vector.as_retriever(search_kwargs={\"k\":5})\n",
        "  return retriever\n",
        "def create_qa_chain(prompt, retriever):\n",
        "\n",
        "  llm = OpenAI(temperature=0.6,model=\"gpt-3.5-turbo-instruct-0914\" )\n",
        "\n",
        "  qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm= llm,\n",
        "    chain_type='stuff',\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True # for source code it sholud be true, if don't need False\n",
        "  )\n",
        "  response = qa_chain(prompt)\n",
        "  return response"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_xlsx_text(xlsx_path):\n",
        "    xlsx_text = \"\"\n",
        "    # Load workbook directly from file path\n",
        "    wb = openpyxl.load_workbook(xlsx_path)\n",
        "\n",
        "    for sheet in wb.worksheets:\n",
        "        for row in sheet.iter_rows():\n",
        "            # Get values in the row, handling None values\n",
        "            row_values = [str(cell.value) if cell.value is not None else '' for cell in row]\n",
        "\n",
        "            # Include rows with at least one non-empty cell\n",
        "            if any(cell_value.strip() for cell_value in row_values):\n",
        "                xlsx_text += ', '.join(row_values) + '\\n'\n",
        "\n",
        "    return xlsx_text"
      ],
      "metadata": {
        "id": "FDf5H-UCoPBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ge7ygMz8GXzG"
      },
      "outputs": [],
      "source": [
        "pdf_path =\"/content/pdf/Supplementary KYC (1).pdf\"\n",
        "# Extract text from the PDF file\n",
        "input_text = extract_text_from_pdf(pdf_path)\n",
        "print(input_text)\n",
        "\n",
        "# Define your data_source_id, file_id, and index_id as a list\n",
        "file_id = '/content/pdf'\n",
        "\n",
        "# Define your file_name based on the input file\n",
        "file_name = os.path.basename(pdf_path)\n",
        "retriever = create_retriver()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CONjGUQAQuIV"
      },
      "outputs": [],
      "source": [
        "prompt =\" which DEPTID has the least bouns? \"\n",
        "response = create_qa_chain(prompt, retriever)\n",
        "llm_response = response\n",
        "print(llm_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2XXmLYtyCTE"
      },
      "outputs": [],
      "source": [
        "def process_llm_response(llm_response):\n",
        "  print(llm_response['result'])\n",
        "  print('\\n\\nSources:')\n",
        "  for source in llm_response[\"source_documents\"]:\n",
        "    print(source.metadata['source'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2es3q3_V3xkm"
      },
      "outputs": [],
      "source": [
        "prompt_template  =\"which DEPTID has the least bouns? \"\n",
        "response = create_qa_chain(prompt, retriever)\n",
        "llm_response = response\n",
        "process_llm_response(llm_response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}